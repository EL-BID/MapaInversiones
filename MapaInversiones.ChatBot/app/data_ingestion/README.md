# üöÄ ChatBot ETL Pipeline

Este directorio contiene el pipeline de **Extracci√≥n, Transformaci√≥n y Carga (ETL)** modular para la ingesta de datos del ChatBot.

El sistema ha sido refactorizado para ser **Config-Driven** (impulsado por configuraci√≥n), lo que significa que agregar nuevas fuentes de datos no requiere (en la mayor√≠a de los casos) tocar c√≥digo Python, sino simplemente editar archivos YAML.

---

## üèóÔ∏è Arquitectura

El pipeline se divide en **5 Etapas (Stages)** secuenciales:

1.  **01_ingest (Ingesta)**:
    *   **Origen**: SQL Server, Archivos CSV/Excel, URLs.
    *   **Destino**: Tablas `stg_` (Staging) en PostgreSQL.
    *   **Funci√≥n**: Copia datos crudos tal cual, truncando la tabla de destino antes de insertar (o limpiando por pa√≠s).
2.  **02_refine (Refinamiento)**:
    *   **Origen**: Tablas `stg_`.
    *   **Destino**: Tablas de Metadatos `sch_tables`, `sch_columns`, `sch_relations`.
    *   **Funci√≥n**: "Refleja" la estructura de la BD, genera descripciones con LLM y embeddings para que el bot entienda el esquema.
3.  **03_dimensions (Dimensiones)**:
    *   **Origen**: Tablas `stg_`.
    *   **Destino**: Tablas dimensionales `dim_sectores`, `dim_territorios`.
    *   **Funci√≥n**: Normaliza entidades (sectores, lugares) para b√∫squedas difusas y cat√°logos.
4.  **04_knowledge (Conocimiento)**:
    *   **Origen**: `raw_fewshots` (CSV/Tabla) y `sch_fewshots`.
    *   **Destino**: `sch_fewshots`.
    *   **Funci√≥n**: Transfiere nuevos few-shots, ejecuta su SQL para llenar `expected_output`, corrige (LLM) las consultas inv√°lidas y prepara todas las columnas/embeddings necesarios para el stage 05.

   Esta etapa recorre `raw_fewshots` para poblar `sch_fewshots`, luego valida y corrige las consultas a trav√©s del LLM (`app/data_ingestion/stages/04_knowledge/validator.py`). Si tienes nuevas few-shots o necesitas re-validar, ejecuta solo esta etapa (`python app/data_ingestion/manage.py run-stage 04_knowledge`); `run-batch` ya la ejecuta despu√©s de las etapas de dim y refine.
5.  **05_publish (Publicaci√≥n)**:
    *   **Origen**: Todo lo anterior (`sch_tables`, `sch_columns`, `sch_fewshots`, `dim_*`).
    *   **Destino**: Tabla Universal `public.documents`.
    *   **Funci√≥n**: Genera el √≠ndice vectorial final usado por el RAG del Chatbot.

---

## üõ†Ô∏è C√≥mo Usar (`manage.py`)

El punto de entrada es el script `manage.py`. Aseg√∫rate de estar en el entorno virtual.

### 1. Comandos B√°sicos

```bash
# Ver ayuda
python app/data_ingestion/manage.py --help

# üì° Probar conexiones a BD (Postgres y SQL Server)
python app/data_ingestion/manage.py test-connections
```

### 2. Ejecutar el Pipeline Completo
Para correr **todas** las etapas en orden (ideal para rerescos nocturnos o despliegues iniciales):

```bash
python app/data_ingestion/manage.py run-batch --country dom
```
*(El flag `--country` es √∫til si tienes l√≥gica de limpieza por pa√≠s, por defecto es 'dom')*

### 3. Ejecutar una Etapa Espec√≠fica
Si solo quieres regenerar los documentos o solo re-ingestar datos crudos:

```bash
# Solo ingesta (Stage 01)
python app/data_ingestion/manage.py run-stage 01_ingest

# Solo publicaci√≥n de documentos (Stage 05)
python app/data_ingestion/manage.py run-stage 05_publish
```

### 4. Ingestar una Sola Fuente
Si agregaste una tabla nueva en el YAML y solo quieres traerla sin correr todo:

```bash
# 'proyectos_dom' es el ID definido en sources.yaml
python app/data_ingestion/manage.py ingest --source-id proyectos_dom
```

---

## ‚öôÔ∏è Configuraci√≥n y Fuentes (`sources.yaml`)

**Aqu√≠ es donde ocurre la magia.**

El archivo de configuraci√≥n principal es:
üìÇ `app/data_ingestion/config/sources.yaml`

### ¬øC√≥mo agregar una nueva fuente?

Simplemente a√±ade una entrada en la lista `sources`.

#### Ejemplo 1: Agregar una Tabla SQL
Supongamos que quieres traer la tabla de "Ejecuci√≥n F√≠sica" desde SQL Server.

```yaml
sources:
  - id: ejecucion_fisica_dom
    type: sql_server
    enabled: true
    connection_ref: AZURE_SQL_MAIN
    country: dom
    destination_table: stg_ejecucion_fisica
    sql_query: |
      SELECT 
        id_proyecto, 
        avance_fisico, 
        fecha_corte,
        'dom' as pais_iso3
      FROM P_MAPAINVERSIONES.dbo.t_ejecucion_fisica
      WHERE pais = 'Republica Dominicana'
```

#### Ejemplo 2: Agregar un CSV Local
Supongamos que tienes un Excel/CSV manual de "C√≥digos SNIP prioritarios".

```yaml
sources:
  - id: codigos_prioritarios
    type: csv_file
    enabled: true
    country: dom
    destination_table: stg_codigos_prioritarios
    path: "data/manual/codigos_snip_2024.csv"
```

### Campos Clave del YAML

| Campo | Descripci√≥n |
| :--- | :--- |
| **`id`** | Nombre √∫nico de la fuente (sin espacios). |
| **`type`** | `sql_server` (consultas en SQL Server) o `csv_file` (archivo local). |
| **`connection_ref`** | Alias usado para resolver un conector preconfigurado (ver abajo). Si se omite, se usa la primera conexi√≥n disponible. |
| **`country`** | Opcional; se usa para limpiar datos previos de ese pa√≠s antes de recargar. |
| **`destination_table`** | Tabla destino en Postgres. Debe ser v√°lida y preferiblemente comenzar con `stg_`. El c√≥digo valida el nombre y hace `DELETE ... WHERE pais_iso3 = :country` si hay pa√≠s. |
| **`sql_query`** | Consulta SQL que se ejecutar√° (solo `sql_server`). |
| **`path`** | Ruta relativa o absoluta del CSV (solo `csv_file`). |
| **`enabled`** | Puedes apagar la fuente sin borrar la entrada. |

### Resoluci√≥n de conectores y CSV

La lista de alias/conn strings vive en `app/data_ingestion/config/settings_etl.py` bajo `ETLSettings.SQL_SERVER_SOURCES`. Por defecto incluye `AZURE_SQL_MAIN`, que apunta a la propiedad `sqlserver_conn_string` de `modules.config.settings`. Si necesitas m√°s or√≠genes, a√±ade nuevos pares `ALIAS: property_name` y actualiza `.env` con la conexi√≥n correspondiente.

Cuando una fuente declara `connection_ref: AZURE_SQL_MAIN`, el controlador de Stage 01 obtiene ese connector y ejecuta la consulta. Si la conexi√≥n no existe, la carga falla con el log correspondiente.

Los CSV se resuelven intentando primero la ruta absoluta, luego `BASE_DIR`, luego `BASE_DIR.parent` y finalmente el `cwd`. Si el archivo no se encuentra, el contenedor se detiene antes de escribir nada.

---

## üß© Dependencias (`dependencies.py`)

Archivo: `app/data_ingestion/config/dependencies.py`

Si creas una tabla nueva (ej. `stg_nueva`), no *necesitas* obligatoriamente tocar este archivo para que la ingesta funcione. 

Sin embargo, si esa tabla es **padre** de otra (por ejemplo, `stg_proyectos` es padre de `stg_territorios` por FK), debes definirlas aqu√≠ para que el sistema sepa en qu√© orden borrarlas y cargarlas.

```python
DEPENDENCIES = {
    "stg_hija": ["stg_padre"],  # stg_padre se carga primero
    ...
}
```

---

## üèóÔ∏è Extensi√≥n del C√≥digo

- **Nuevos Connectores**: `app/data_ingestion/core/connectors.py` (ej. agregar MySQL o Oracle).
- **L√≥gica de Transformaci√≥n**:
    - Si es limpieza ligera, hazlo en la SQL del `sources.yaml`.
    - Si es compleja (normalizaci√≥n, IA), agrega un script en `app/data_ingestion/stages/02_refine/` o `03_dimensions/` y reg√≠stralo en el controlador de esa etapa.

## üìù Variables de Entorno

El sistema usa las mismas variables que el Chatbot (`.env`):
- `POSTGRES_CONN_STRING`: Conexi√≥n a la BD destino.
- `SQLSERVERDATA_CONN_STRING`: Conexi√≥n a la BD origen principal.
- Credenciales de Azure OpenAI (para generar descripciones y embeddings).

## ‚öôÔ∏è Configuraci√≥n Avanzada

El archivo `app/data_ingestion/config/settings_etl.py` contiene par√°metros configurables del ETL:

```python
class ETLSettings(BaseSettings):
    # Database Connection Pools
    POSTGRES_POOL_SIZE: int = 10
    SQLSERVER_POOL_SIZE: int = 5
    
    # ETL Processing Settings
    PREVIEW_ROWS: int = 15              # Filas a guardar en few-shot validation
    SQL_EXECUTION_TIMEOUT: int = 30     # Timeout (segundos) para ejecuci√≥n SQL
    LLM_TEMPERATURE: float = 0.0        # Temperatura para llamadas LLM
```

Puedes sobrescribir estos valores:
- Editando directamente `settings_etl.py`
- Definiendo variables de entorno (ej: `ETL_PREVIEW_ROWS=20`)
- Para producci√≥n, ajusta `SQL_EXECUTION_TIMEOUT` seg√∫n queries complejas

## üìù Logs

El sistema utiliza **Loguru** para la gesti√≥n de logs rotativos.

- **Ubicaci√≥n por defecto**: `logs/etl.log` (en la ra√≠z del proyecto).
- **Configuraci√≥n**:
  - **Rotaci√≥n**: 10 MB (crea un archivo nuevo al llegar a este tama√±o).
  - **Retenci√≥n**: 7 d√≠as (elimina logs m√°s antiguos).
  - **Nivel**: `DEBUG` en archivo, `INFO` en consola.

Puedes modificar esta configuraci√≥n editando `app/data_ingestion/manage.py`:

```python
logger.add("logs/etl.log", rotation="10 MB", retention="7 days", level="DEBUG")
```

---

## ‚è∞ Automatizaci√≥n (Crontab)

Para ejecutar el pipeline autom√°ticamente (por ejemplo, todas las ma√±anas a las 6:00 AM), puedes usar **cron**.

1. Abre el editor de cron en tu servidor/m√°quina:
   ```bash
   crontab -e
   ```

2. Agrega una l√≠nea como la siguiente (ajustando tus rutas):

   ```bash
   # Ejecuta el ETL completo para Rep√∫blica Dominicana (dom) a las 6:00 AM todos los d√≠as
   0 6 * * * cd /home/usuario/ChatBot_v2 && /home/usuario/ChatBot_v2/venv/bin/python app/data_ingestion/manage.py run-batch --country dom >> logs/cron_etl.log 2>&1
   ```

**Recomendaciones:**
- **Rutas Absolutas**: Siempre usa la ruta completa al binario de `python` dentro de tu entorno virtual (ej. `/venv/bin/python`).
- **Logs**: Redirige siempre la salida (`>>`) a un archivo para poder debuggear si algo falla silenciosamente.
- **Docker**: Si corres con Docker, el cron debe ser en el host llamando a `docker exec`:
  ```bash
  0 6 * * * docker exec chatbot-backend python app/data_ingestion/manage.py run-batch --country dom
  ```

---

## üß© Addon: Generador de Few-Shots (`app/gen`)

Adem√°s del pipeline de ingesta batch, existe un sistema interactivo ("Addon") para **generar y curar Few-Shots din√°micamente** bas√°ndose en preguntas reales de usuarios.

Ubicaci√≥n: `app/gen/`

Este m√≥dulo permite:
1.  **Leer preguntas reales** sin respuesta de la base de datos (`questions_mapainv_chat`).
2.  **Generar SQL** autom√°ticamente usando un LLM.
3.  **Validar y Corregir** la SQL interactivamente.
4.  **Insertar** el resultado como un nuevo *few-shot* en `sch_fewshots`, que luego es procesado por el Stage 04 del ETL.

### Uso R√°pido
```bash
# Ejecutar el CLI interactivo
python app/gen/cli.py
```

Para m√°s detalles, consultar el `README.md` dentro de `app/gen`.
