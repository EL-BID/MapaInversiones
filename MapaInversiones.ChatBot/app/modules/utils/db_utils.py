from __future__ import annotations
from sqlalchemy import text
import json
from sqlalchemy.orm import Session as SessionType
import re
import time
from threading import Lock
from datetime import datetime
from modules.config import settings
from typing import Dict, Tuple, Set, List, Optional, Any
from psycopg2 import extensions as pg_ext
from database.async_postgres import sync_db_acquire
from database.chatbot_engine import chatbot_engine


"""
This script:
- Provides utilities to execute SQL queries against a PostgreSQL database.
- Handles different types of SQL commands: SELECT, INSERT, UPDATE.
- Transforms exact match expressions using `process_text` into partial match searches with ILIKE.
- Includes functions to clean and normalize SQL queries, especially those generated by LLMs.
- Manages database connections using SQLAlchemy with connection pooling.
- Provides a function to log and fix SQL queries using SQLFluff.
- Contains a function to add questions to a specific table and retrieve the generated ID.
- Contains a function to retrieve question history by session ID.

"""

# ── Silence sqlfluff logs even if you use Loguru ─────────────
import logging
from loguru import logger

for name in (
    "sqlfluff",  # logger raíz del paquete
    "sqlfluff.cli.commands",  # sub-logger del CLI
    "sqlfluff.rules",  # reglas individuales
):
    log = logging.getLogger(name)
    log.handlers.clear()  # elimina handlers que sqlfluff añadió
    log.propagate = False  # no re-envía al root logger
    log.setLevel(logging.CRITICAL)  # solo mostraría mensajes críticos
# ──────────────────────────────────────────────────────────────
from pathlib import Path
import sqlfluff
from modules.utils.markdown_utils import markdown_to_html

CFG_PATH = Path(__file__).resolve().parent.parent.parent / "sqlfluff"
logger.info(f"---- SQLFluff config path: {CFG_PATH}")
if not CFG_PATH.exists():
    logger.error(f"---- SQLFluff config path does not exist: {CFG_PATH}")
EXCLUDED = ["LT05", "AL07", "LT12", "LT13"]
_SQL_SPACES = re.compile(r"\s+", re.I)

# ═════════════════════════════ utilidades ════════════════════════════════
# ➊ Evita que Loguru intercepte los logs de SQLFluff
logger.disable("sqlfluff")  # raíz del paquete
logger.disable("sqlfluff.cli.commands")  # CLI interna


def lint_and_fix_sql(sql: str):
    fixed = sqlfluff.fix(
        sql,
        dialect="postgres",
        config_path=str(CFG_PATH),
        exclude_rules=EXCLUDED,
    )
    violations = sqlfluff.lint(
        fixed,
        dialect="postgres",
        config_path=str(CFG_PATH),
        exclude_rules=EXCLUDED,
    )

    errs = [f"SQLFluff {v['code']}: {v['description']}" for v in violations]

    return fixed


def _fix_round_numeric_cast(sql: str) -> str:
    """
    Heuristic fix for PostgreSQL error: function round(double precision, integer) does not exist.
    Wraps the first argument of ROUND in CAST((...) AS NUMERIC) if it contains division or float operations.

    Pattern: ROUND( <expression> , <integer> )
    Replacement: ROUND(CAST((<expression>) AS NUMERIC), <integer>)

    IMPROVED: Now handles expressions with commas (NULLIF, COALESCE, etc.) by wrapping
    the entire expression in parentheses before casting.
    """
    if "ROUND" not in sql.upper():
        return sql

    # Match ROUND( <content> , <precision> )
    # Using non-greedy match (.+?) stops at the first comma at the same nesting level
    # This is imperfect but handles most LLM-generated patterns
    pattern = re.compile(r"ROUND\s*\((.+?)\s*,\s*(\d+)\s*\)", re.IGNORECASE | re.DOTALL)

    def replacer(match):
        content = match.group(1).strip()
        precision = match.group(2)

        # Evitar tocar expresiones complejas con NULLIF/COALESCE u otras comas internas
        if "," in content or "NULLIF(" in content.upper():
            return match.group(0)

        # Skip if already has AS NUMERIC cast
        if "AS NUMERIC" in content.upper() or "AS numeric" in content:
            return match.group(0)

        # Apply fix if content contains division or floating-point numbers
        # These typically result in double precision
        if "/" in content or ".0" in content:
            # Wrap entire expression in parentheses and cast to NUMERIC
            # This handles complex expressions including NULLIF(x, 0), COALESCE, etc.
            return f"ROUND(CAST(({content}) AS NUMERIC), {precision})"

        return match.group(0)

    return pattern.sub(replacer, sql)


def lint_and_fix_sql(sql: str):
    # 1. Run standard SQLFluff fix
    fixed = sqlfluff.fix(
        sql,
        dialect="postgres",
        config_path=str(CFG_PATH),
        exclude_rules=EXCLUDED,
    )

    # 2. Apply heuristic fix for ROUND() type error
    fixed = _fix_round_numeric_cast(fixed)

    violations = sqlfluff.lint(
        fixed,
        dialect="postgres",
        config_path=str(CFG_PATH),
        exclude_rules=EXCLUDED,
    )

    errs = [f"SQLFluff {v['code']}: {v['description']}" for v in violations]

    return fixed


# ═══════════════════════════════════════════════════════════════════════════
# POOL UNIFICADO: usar el engine central de database/postgres.py
# Esto evita tener 2 pools de conexiones compitiendo por recursos
# ═══════════════════════════════════════════════════════════════════════════
from database.postgres import postgres_engine as engine, PostgresSessionLocal as Session

logger.info("db_utils - Using unified connection pool from database/postgres.py")


class SqlGuardException(Exception):
    """Base exception for SQL guardrails."""


class ForbiddenStatementError(SqlGuardException):
    """Raised when a statement uses disallowed constructs."""


class TableNotAllowedError(SqlGuardException):
    """Raised when a statement targets a table outside the whitelist."""


_READ_TABLES_TTL_SECONDS = 30 * 60
_ALLOWED_READ_TABLES_CACHE: Optional[Set[str]] = None
_ALLOWED_READ_TS: float = 0.0
_READ_TABLES_LOCK = Lock()
_ALLOWED_INTERNAL_READ_TABLES = {"questions_mapainv_chat"}
_ALLOWED_WRITE_TABLES = {"questions_mapainv_chat", "user_feedback"}
_FORBIDDEN_KEYWORDS = {
    "drop",
    "truncate",
    "alter",
    "grant",
    "revoke",
    "create",
    "replace",
    "attach",
    "detach",
    "vacuum",
    "analyze",
    "comment",
}
_SQL_COMMENT_RE = re.compile(r"(--[^\n]*\n)|(/\*.*?\*/)", re.DOTALL)
_STRING_LITERAL_RE = re.compile(r"('([^']|'')*')|(\"([^\"]|\"\")*\")")
_TABLE_TOKEN_RE_TEMPLATE = r"\b(?:{clauses})\s+([A-Za-z_][\w\.]*|\"[^\"]+\")"
_READ_QUERY_TYPES = {"SELECT", "WITH", "SHOW", "EXPLAIN", "DESCRIBE"}


def get_table_name(sql_query):
    """Extrae el nombre de la tabla de una consulta SQL."""
    match = re.search(
        r"(INTO|UPDATE|FROM|DELETE\s+FROM)\s+([\w.]+)", sql_query, re.IGNORECASE
    )
    return match.group(2) if match else None


def _refresh_allowed_read_tables(force: bool = False) -> Set[str]:
    """
    Load table names from sch_tables, caching them in memory with TTL.
    """
    global _ALLOWED_READ_TABLES_CACHE, _ALLOWED_READ_TS
    now = time.time()
    if (
        not force
        and _ALLOWED_READ_TABLES_CACHE is not None
        and now - _ALLOWED_READ_TS < _READ_TABLES_TTL_SECONDS
    ):
        return set(_ALLOWED_READ_TABLES_CACHE)

    with _READ_TABLES_LOCK:
        if (
            not force
            and _ALLOWED_READ_TABLES_CACHE is not None
            and time.time() - _ALLOWED_READ_TS < _READ_TABLES_TTL_SECONDS
        ):
            return set(_ALLOWED_READ_TABLES_CACHE)

        try:
            with engine.connect() as conn:
                rows = (
                    conn.execute(
                        text("SELECT table_name FROM sch_tables WHERE is_valid = TRUE")
                    )
                    .scalars()
                    .all()
                )
            allowed = {str(row).strip().lower() for row in rows if row}
            _ALLOWED_READ_TABLES_CACHE = allowed
            _ALLOWED_READ_TS = time.time()
            return set(allowed)
        except Exception as exc:
            logger.warning("No se pudo refrescar sch_tables: {}", exc)
            return set(_ALLOWED_READ_TABLES_CACHE or set())


def _get_allowed_select_tables() -> Set[str]:
    base = _refresh_allowed_read_tables()
    return base.union(_ALLOWED_INTERNAL_READ_TABLES)


def _strip_sql_comments(sql: str) -> str:
    return _SQL_COMMENT_RE.sub(" ", sql)


def _strip_string_literals(sql: str) -> str:
    return _STRING_LITERAL_RE.sub(" ", sql)


def _has_multiple_statements(sql: str) -> bool:
    sanitized = sql.strip()
    if ";" not in sanitized:
        return False
    first = sanitized.find(";")
    return first < len(sanitized) - 1


def _extract_tables_from_clauses(sql: str, clauses: Tuple[str, ...]) -> Set[str]:
    if not clauses:
        return set()
    pattern = re.compile(
        _TABLE_TOKEN_RE_TEMPLATE.format(clauses="|".join(clauses)),
        re.IGNORECASE,
    )
    tables: Set[str] = set()
    for match in pattern.finditer(sql):
        token = match.group(1).strip()
        if not token or token.startswith("("):
            continue

        # Detectar si es una función (token seguido de '(')
        # match.end() es la posición donde termina el match del token
        # Verificamos el carácter inmediatamente después del token
        token_end_pos = match.end()
        # Buscar el siguiente carácter no-espacio después del token
        remaining = sql[token_end_pos:].lstrip()
        if remaining.startswith("("):
            # Es una función, no una tabla - saltarla
            continue

        # Ignorar pseudotablas/constantes de fecha/tiempo/usuario/esquema (no son tablas reales)
        lowered = token.lower()
        pseudo_tokens = {
            "current_date",
            "current_timestamp",
            "current_time",
            "localtimestamp",
            "localtime",
            "now",
            "timeofday",
            "clock_timestamp",
            "current_user",
            "session_user",
            "user",
            "current_schema",
        }
        if (
            lowered in pseudo_tokens
            or lowered.startswith("current_")
            or lowered.startswith("local")
        ):
            continue

        token = token.strip('"')
        tables.add(token.split(".")[-1].lower())
    return tables


def _extract_single_table(sql: str, clause: str) -> Optional[str]:
    matches = _extract_tables_from_clauses(sql, (clause,))
    return next(iter(matches)) if matches else None


def _ensure_allowed_select_tables(tables: Set[str]):
    allowed = _get_allowed_select_tables()
    disallowed = {tbl for tbl in tables if tbl and tbl not in allowed}
    if disallowed:
        raise TableNotAllowedError(
            f"Tablas no permitidas en consulta: {', '.join(sorted(disallowed))}"
        )


def _ensure_allowed_write_table(table: Optional[str]):
    if not table:
        raise TableNotAllowedError("No se pudo determinar la tabla destino.")
    normalized = table.split(".")[-1].lower()
    if normalized not in _ALLOWED_WRITE_TABLES:
        raise TableNotAllowedError(
            f"La tabla '{normalized}' no está autorizada para escritura."
        )


def _enforce_sql_guardrails(sql: str, query_type: str) -> None:
    """
    Apply keyword, multi-statement and table whitelist guardrails.
    """
    if not sql:
        raise ForbiddenStatementError("Consulta vacía.")

    sql_no_comments = _strip_sql_comments(sql)
    stripped_literals = _strip_string_literals(sql_no_comments)

    if _has_multiple_statements(stripped_literals):
        raise ForbiddenStatementError(
            "Sólo se permite una sentencia SQL por ejecución."
        )

    lowered = stripped_literals.lower()
    for keyword in _FORBIDDEN_KEYWORDS:
        if re.search(rf"\b{keyword}\b", lowered):
            raise ForbiddenStatementError(
                f"Uso de palabra clave no permitida: '{keyword}'."
            )

    effective_type = query_type
    if effective_type == "EXPLAIN":
        remainder = sql_no_comments[len("EXPLAIN") :].lstrip()
        if remainder:
            effective_type = remainder.split()[0].upper()
        else:
            effective_type = "SELECT"

    if effective_type in _READ_QUERY_TYPES:
        # if effective_type not in {"SHOW", "DESCRIBE"}:
        #     tables = _extract_tables_from_clauses(sql_no_comments, ("FROM", "JOIN"))
        #     _ensure_allowed_select_tables(tables)
        return

    if effective_type == "INSERT":
        table = _extract_single_table(sql_no_comments, "INTO")
        _ensure_allowed_write_table(table)
        return

    if effective_type == "UPDATE":
        table = _extract_single_table(sql_no_comments, "UPDATE")
        _ensure_allowed_write_table(table)
        return

    raise ForbiddenStatementError(f"Operación no permitida: {effective_type}")


def _reset_failed_transaction(db: Session) -> None:
    """
    Intenta limpiar cualquier transacción previa en error para evitar heredar
    conexiones abortadas desde el pool.
    """
    try:
        # Hacer rollback siempre es seguro; si no hay tx activa, no pasa nada.
        db.rollback()
    except Exception:
        pass

    try:
        conn = db.connection()
        raw = getattr(conn, "connection", None) or getattr(
            conn, "driver_connection", None
        )
        if raw and hasattr(raw, "get_transaction_status"):
            status = raw.get_transaction_status()
            # Estados de psycopg2.extensions:
            # 0=IDLE, 1=ACTIVE, 2=INTRANS, 3=INERROR, 4=UNKNOWN, 5=PREPARED
            if status in (
                pg_ext.TRANSACTION_STATUS_INERROR,
                pg_ext.TRANSACTION_STATUS_UNKNOWN,
            ):
                logger.warning(
                    "SQL ▸ RESET failed transaction status=%s → rollback()", status
                )
                db.rollback()
    except Exception as exc:  # pragma: no cover - defensivo
        logger.warning("SQL ▸ RESET failed transaction check error={}", exc)


def add_question(
    pais,
    pregunta,
    sql_query,
    data_answer,
    datetime,
    source,
    profile,
    comments,
    totalSearch,
    totalLikes,
    totalTokenInput,
    totalTokenOutput,
    userSession,
    trace,
    QuestionSummary,
):
    """
    Inserta una pregunta en la base de datos y devuelve el ID generado.
    """
    logger.info("Conectando a PostgreSQL..")
    answer_id = 0

    def _preview(value, max_len=200):
        if value is None:
            return None
        text = str(value).strip()
        if len(text) > max_len:
            text = text[: max_len - 3] + "..."
        return text

    try:
        if sql_query:
            insert_stmt = text(
                """
                INSERT INTO questions_mapainv_chat
                (Pais, Question, Query, Answer, Date, Source, Profile, Comments,
                 TotalSearch, TotalLikes, TotalTokenInput, TotalTokenOutput,
                 UserSession, Trace, QuestionSummary)
                VALUES
                (:pais, :pregunta, :sql_query, :data_answer, :datetime,
                 :source, :profile, :comments, :totalSearch, :totalLikes,
                 :totalTokenInput, :totalTokenOutput, :userSession, :trace,
                 :QuestionSummary)
                RETURNING id;
                """
            )

            params = {
                "pais": pais,
                "pregunta": pregunta,
                "sql_query": sql_query,
                "data_answer": data_answer,
                "datetime": datetime,
                "source": source,
                "profile": profile,
                "comments": comments,
                "totalSearch": totalSearch,
                "totalLikes": totalLikes,
                "totalTokenInput": totalTokenInput,
                "totalTokenOutput": totalTokenOutput,
                "userSession": userSession,
                "trace": trace,
                "QuestionSummary": QuestionSummary,
            }

            logger.info(
                "ADD_QUESTION ▸ preparando inserción pais={} session={} question='{}' sql='{}'",
                pais,
                userSession,
                _preview(pregunta, 160),
                _preview(sql_query, 160),
            )

            with Session() as db:
                result = db.execute(insert_stmt, params)
                db.commit()
                answer_id = result.scalar() or 0
                logger.info(
                    "ADD_QUESTION ▸ insertado id={} session={} question='{}'",
                    answer_id,
                    userSession,
                    _preview(pregunta, 120),
                )

    except Exception as e:
        logger.error(
            "Error en `add_question()` pais={} session={} question='{}': {}",
            pais,
            userSession,
            _preview(pregunta, 120),
            e,
        )
    finally:
        return answer_id


def add_custom_feedback(
    answer_id,
    user_email,
    user_question,
    feedback_text,
    user_session=None,
):
    """
    Inserta feedback personalizado de usuario en la tabla `user_feedback`.
    Si la tabla no existe, la función intentará ejecutar el INSERT y dejará
    que Postgres reporte el error (para que el deploy/DB migration lo gestione).

    Retorna el id insertado o 0 si ya existe para (answer_id, session_id).
    Propaga la excepción en otros errores.
    """
    logger.info("Conectando a PostgreSQL para insertar user feedback")

    # Basic sanitization / validation to reduce risk and bound sizes
    def _sanitize_text(val, max_len=2000):
        if val is None:
            return None
        s = str(val)
        # remove NULLs and control chars except newline/tab
        s = re.sub(r"[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]+", " ", s)
        s = s.strip()
        if max_len and len(s) > max_len:
            s = s[:max_len]
        return s

    # Coerce answer_id to int or None
    try:
        aid = int(answer_id) if answer_id is not None and answer_id != "" else None
    except Exception:
        aid = None

    email_s = _sanitize_text(user_email, max_len=254)
    question_s = _sanitize_text(user_question, max_len=1000)
    feedback_s = _sanitize_text(feedback_text, max_len=2000)
    session_s = _sanitize_text(user_session, max_len=128)

    # Requires a matching UNIQUE index/constraint on (answer_id, session_id).
    insert_stmt = text(
        """
        INSERT INTO user_feedback
        (answer_id, user_email, user_question, feedback_text, session_id, created_at)
        VALUES
        (:answer_id, :user_email, :user_question, :feedback_text, :session_id, now())
        ON CONFLICT (answer_id, session_id) DO NOTHING
        RETURNING id;
        """
    )

    params = {
        "answer_id": aid,
        "user_email": email_s,
        "user_question": question_s,
        "feedback_text": feedback_s,
        "session_id": session_s,
    }

    logger.info(
        "ADD_CUSTOM_FEEDBACK ▸ preparando inserción answer_id={} session={} email={} question='{}'",
        aid,
        session_s,
        email_s,
        _sanitize_text(user_question, max_len=80),
    )

    with Session() as db:
        try:
            # SQLAlchemy text + bound params => safe against SQL injection
            result = db.execute(insert_stmt, params)
            db.commit()
            inserted_id = result.scalar() or 0
            if inserted_id:
                logger.info(
                    "ADD_CUSTOM_FEEDBACK ▸ insertado id={} answer_id={} session={}",
                    inserted_id,
                    aid,
                    session_s,
                )
            else:
                logger.warning(
                    "ADD_CUSTOM_FEEDBACK ▸ duplicado answer_id={} session={}",
                    aid,
                    session_s,
                )
            return inserted_id
        except Exception as e:
            db.rollback()
            logger.error(
                "Error en `add_custom_feedback()` answer_id={} session={} email={}: {}",
                aid,
                session_s,
                email_s,
                e,
            )
            raise


# ------------------------------------------------------------
# Quita process_text(col) en SELECT / GROUP BY / ORDER BY
# pero lo deja intacto en WHERE y filtros.
# ------------------------------------------------------------
_SELECT_CLAUSE = re.compile(
    r"^\s*select\s+(?P<body>.+?)\s+from\s",
    flags=re.IGNORECASE | re.DOTALL,
)


_COLUMN_TOKEN = r"(?:[A-Za-z_][\w]*\.)?[A-Za-z_][\w]*"
_COLUMN_ILIKE_VALUE_RE = re.compile(
    rf"(?P<column>{_COLUMN_TOKEN})\s+(?P<op>ILIKE|LIKE)\s+(?P<value>'%\s*\|\|\s*process_text\('([^']+)'\)\s*\|\|\s*%')",
    flags=re.IGNORECASE,
)
_COLUMN_ILIKE_PLAIN_PT_RE = re.compile(
    rf"(?P<column>{_COLUMN_TOKEN})\s+(?P<op>ILIKE|LIKE)\s+process_text\('(?P<raw>[^']+)'\)",
    flags=re.IGNORECASE,
)
_COLUMN_EQ_PT_RE = re.compile(
    rf"(?P<column>{_COLUMN_TOKEN})\s*=\s*process_text\('(?P<raw>[^']+)'\)",
    flags=re.IGNORECASE,
)
_COLUMN_ILIKE_RAW_LITERAL_RE = re.compile(
    rf"(?P<column>{_COLUMN_TOKEN})\s+(?P<op>ILIKE|LIKE)\s+'(?P<raw>(?:''|[^'])*)'",
    flags=re.IGNORECASE,
)
_COLUMN_EQ_RAW_LITERAL_RE = re.compile(
    rf"(?P<column>{_COLUMN_TOKEN})\s*=\s*'(?P<raw>(?:''|[^'])*)'",
    flags=re.IGNORECASE,
)


def _wrap_column_with_process_text(column: str) -> str:
    column_clean = column.strip()
    if column_clean.lower().startswith("process_text("):
        return column_clean
    unwrapped = _unwrap_text_normalizer(column_clean)
    if unwrapped.lower().startswith("process_text("):
        return unwrapped
    return f"process_text({unwrapped})"


def _unwrap_text_normalizer(expr: str) -> str:
    """
    Remove common normalizer functions such as LOWER(), UPPER(), INITCAP(), TRIM()
    that are typically applied before comparisons.
    """
    if not expr:
        return expr

    current = expr.strip()
    # Quickly skip special cases we should not touch.
    lowered = current.lower()
    if "regexp_split_to_table" in lowered or "w.token" in lowered:
        return current

    pattern = re.compile(r"(?is)^(?:lower|upper|initcap|trim)\(\s*(.+?)\s*\)$")
    prev = None
    while current != prev:
        prev = current
        match = pattern.match(current)
        if not match:
            break
        current = match.group(1).strip()

    # Remove redundant surrounding parentheses.
    while current.startswith("(") and current.endswith(")"):
        inner = current[1:-1].strip()
        if not inner:
            break
        current = inner

    return current


def _unwrap_literal_normalizer(raw: str) -> str:
    """
    Remove normalizer wrappers (LOWER/UPPER/TRIM/INITCAP) around string literals.
    """
    if not raw:
        return raw
    pattern = re.compile(
        r"(?is)^(?:lower|upper|initcap|trim)\(\s*'((?:''|[^'])*)'\s*\)$"
    )
    current = raw.strip()
    prev = None
    while current != prev:
        prev = current
        match = pattern.match(current)
        if not match:
            break
        current = match.group(1)
    return current


def _should_skip_text_enforcement(column: str) -> bool:
    col = column.strip().lower()
    if not col:
        return True

    # Skip for ISO codes and identifiers that should be exact matches
    if "pais_iso3" in col or "codigocanal_proyecto" in col or "codigo_" in col:
        return True

    if "w.token" in col or col.endswith(".token"):
        return True
    if "regexp_split_to_table" in col:
        return True
    return False


def enforce_process_text_filters(sql: str) -> str:
    """
    Ensure both sides of textual comparisons use process_text().
    Transforms patterns such as `p.col ILIKE '%' || process_text('x') || '%'`
    into `process_text(p.col) ILIKE '%' || process_text('x') || '%'`.
    Also normalizes direct equals / plain process_text calls.
    """

    def _format_process_text_value(raw: str) -> str:
        # Normalize literal by unwrapping LOWER/UPPER/TRIM wrappers and cleaning quotes.
        normalized_raw = _unwrap_literal_normalizer(raw)
        value = normalized_raw.replace("''", "'").strip()
        if value:
            # Remove leading/trailing % so we reconstruct them consistently.
            stripped = value.strip("%")
            if stripped:
                value = stripped
        escaped = value.replace("'", "''")
        return f"'%' || process_text('{escaped}') || '%'"

    def repl_ilike_value(match: re.Match) -> str:
        column_raw = match.group("column")
        if _should_skip_text_enforcement(column_raw):
            return match.group(0)
        column = _wrap_column_with_process_text(column_raw)
        op = match.group("op").upper()
        value = match.group("value")
        return f"{column} {op} {value}"

    def repl_ilike_plain(match: re.Match) -> str:
        column_raw = match.group("column")
        if _should_skip_text_enforcement(column_raw):
            return match.group(0)
        column = _wrap_column_with_process_text(column_raw)
        op = match.group("op").upper()
        raw = match.group("raw")
        value = _format_process_text_value(raw)
        return f"{column} {op} {value}"

    def repl_eq(match: re.Match) -> str:
        column_raw = match.group("column")
        if _should_skip_text_enforcement(column_raw):
            return match.group(0)
        column = _wrap_column_with_process_text(column_raw)
        raw = match.group("raw")
        value = _format_process_text_value(raw)
        return f"{column} ILIKE {value}"

    def repl_ilike_literal(match: re.Match) -> str:
        column_raw = match.group("column")
        if _should_skip_text_enforcement(column_raw):
            return match.group(0)
        column = _wrap_column_with_process_text(column_raw)
        raw = match.group("raw")
        value = _format_process_text_value(raw)
        return f"{column} ILIKE {value}"

    def repl_eq_literal(match: re.Match) -> str:
        column_raw = match.group("column")
        if _should_skip_text_enforcement(column_raw):
            return match.group(0)
        column = _wrap_column_with_process_text(column_raw)
        raw = match.group("raw")
        literal = _unwrap_literal_normalizer(raw).replace("''", "'").strip()
        escaped = literal.replace("'", "''")
        return f"{column} = process_text('{escaped}')"

    sql = _COLUMN_ILIKE_RAW_LITERAL_RE.sub(repl_ilike_literal, sql)
    sql = _COLUMN_EQ_RAW_LITERAL_RE.sub(repl_eq_literal, sql)
    sql = _COLUMN_ILIKE_VALUE_RE.sub(repl_ilike_value, sql)
    sql = _COLUMN_ILIKE_PLAIN_PT_RE.sub(repl_ilike_plain, sql)
    sql = _COLUMN_EQ_PT_RE.sub(repl_eq, sql)
    return sql


def strip_process_text_in_select(sql_query: str) -> str:
    m = _SELECT_CLAUSE.search(sql_query)
    if not m:
        return sql_query

    body = m.group("body")

    # Sólo desenvuelve: process_text(col)  ->  col
    body_clean = re.sub(
        r"process_text\(\s*([a-zA-Z_][\w\.]*)\s*\)",  # función
        r"\1",  # sólo la columna
        body,
        flags=re.IGNORECASE,
    )
    return sql_query.replace(body, body_clean, 1)


def normalize_territorial_expressions(sql_query: str) -> str:
    """
    Limpia expresiones como 'provincia de Samaná', 'departamento de La Vega', etc.,
    dentro de cláusulas SQL con:
        process_text(campo) ILIKE '%' || process_text('valor') || '%'

    Reemplaza el valor manteniendo la estructura original del SQL.

    Solo afecta valores dentro de cláusulas ILIKE y con formato process_text.
    """
    # Regex: captura campo y valor dentro de la estructura:
    # process_text(campo) ILIKE '%' || process_text('valor') || '%'
    pattern = re.compile(
        r"process_text\(([^)]+)\)\s+ILIKE\s+'%'\s*\|\|\s*process_text\('([^']+)'\)\s*\|\|\s*'%'",
        flags=re.IGNORECASE,
    )

    # Prefijos a eliminar (solo al comienzo del valor)
    territorial_prefix = re.compile(
        r"^(provincia|departamento|municipio|región|region)\s+(de(l)?|la|el)?\s*",
        flags=re.IGNORECASE,
    )

    def clean_territorial(match):
        field = match.group(1)
        raw_value = match.group(2).strip()
        # Limpieza del valor
        cleaned_value = territorial_prefix.sub("", raw_value).strip()
        return (
            f"process_text({field}) ILIKE '%' || process_text('{cleaned_value}') || '%'"
        )

    sql_query = pattern.sub(clean_territorial, sql_query)
    return sql_query


def remove_process_text_from_numeric_fields(sql_query: str) -> str:
    """
    Quita process_text() de columnas numéricas para evitar errores como:
    'function process_text(bigint) does not exist'.
    Solo actúa si la función envuelve columnas numéricas conocidas.
    """
    numeric_fields = [
        "anio_fechafin_proyecto",
        "anio_fechainicio_proyecto",
        "duracion_proyecto",
        "valor_proyecto",
        "porcentajeavancefinanciero_proyecto",
        "id_proyecto",
    ]
    for field in numeric_fields:
        # process_text(anio_fechafin_proyecto) → anio_fechafin_proyecto
        sql_query = re.sub(
            rf"process_text\(\s*{field}\s*\)", field, sql_query, flags=re.IGNORECASE
        )
    return sql_query


def expand_ilike_keywords(
    sql_query: str, min_words: int = 2, min_length: int = 4
) -> str:
    """
    Expande ILIKE con process_text(...) para frases largas, descomponiéndolas en varias condiciones OR
    si la cantidad de palabras útiles supera `min_words`.
    Se ignoran palabras menores a `min_length`.
    """

    def replacer(match):
        column = match.group(1)
        value = match.group(2)
        words = [
            w.strip() for w in value.strip().split() if len(w.strip()) >= min_length
        ]
        if len(words) >= min_words:
            conditions = [
                f"process_text({column}) ILIKE '%' || process_text('{word}') || '%'"
                for word in words
            ]
            return f"({' OR '.join(conditions)})"
        return match.group(0)

    pattern = r"process_text\((\w+)\)\s+ILIKE\s+'%'\s+\|\|\s+process_text\('([^']+)'\)\s+\|\|\s+'%'"
    return re.sub(pattern, replacer, sql_query, flags=re.IGNORECASE)


# Reemplazo para process_text(...) IN ('a', 'b') → ILIKE con OR
def replace_in_clause_ilike(sql_query):
    pattern = r"process_text\(([^)]+)\)\s+IN\s*\(\s*'([^']+)'(?:\s*,\s*'([^']+)')*\s*\)"

    def expand_ilike(match):
        column = match.group(1)
        values = re.findall(r"'([^']+)'", match.group(0))
        conditions = [
            f"process_text({column}) ILIKE '%' || process_text('{val}') || '%'"
            for val in values
        ]
        return "(" + " OR ".join(conditions) + ")"

    return re.sub(pattern, expand_ilike, sql_query, flags=re.IGNORECASE)


def unwrap_process_text_with_alias(sql: str) -> str:
    """
    Sustituye   process_text(col) AS alias   →   col AS alias
    (ignora alias implícitos para evitar confusión con ILIKE, FROM…)
    """
    pattern = re.compile(
        r"""
        process_text\(\s*(?P<col>[a-zA-Z_][\w\.]*)\s*\)   # columna
        \s+AS\s+(?P<alias>"[^"]+"|\w+)                    # AS alias
        """,
        flags=re.IGNORECASE | re.VERBOSE,
    )
    return pattern.sub(r"\g<col> AS \g<alias>", sql)


def _ensure_order_before_limit(sql: str) -> str:
    """
    Detecta patrones incorrectos del tipo `... LIMIT n ORDER BY ...` y los
    reordena a `ORDER BY ... LIMIT n`. Sólo actúa cuando LIMIT aparece antes de
    ORDER BY en la misma sentencia.
    """
    pattern = re.compile(
        r"(?is)(.*?)(\s+limit\s+\d+(?:\s+offset\s+\d+)?)\s+(order\s+by\s+.+)",
    )
    match = pattern.fullmatch(sql.strip())
    if not match:
        match = pattern.search(sql)

    if not match:
        return sql

    prefix = match.group(1).rstrip()
    limit_clause = match.group(2).strip()
    order_clause = match.group(3).strip()
    suffix = sql[match.end() :]

    reordered = " ".join(filter(None, [prefix, order_clause, limit_clause]))
    return f"{reordered}{suffix}".strip()


# ---- Preflight fix for LLM-generated UNION/UNION ALL with per-branch ORDER BY/LIMIT ----
_UNION_TOKEN_RE = re.compile(r"\bUNION(?:\s+ALL)?\b", re.IGNORECASE)
_ORDER_BEFORE_UNION_RE = re.compile(
    r"""
    (                               # start capture group 1 = branch body
      (?:order\s+by\s+[^;()]*?      #   ORDER BY ... (non-greedy, stop before paren/semicolon)
         (?:\s+limit\s+\d+)?        #   optional LIMIT n
      )\s*
    )(?=\bUNION\b)                  # look‑ahead: immediately before UNION / UNION ALL
    """,
    re.IGNORECASE | re.DOTALL | re.VERBOSE,
)


def _fix_unions_for_preview(sql: str) -> str:
    """
    Lightweight preflight fix for LLM‑generated UNION / UNION ALL statements.

    Problem observed:
        The model often emits:
            SELECT ... ORDER BY ... LIMIT 1 UNION ALL SELECT ... ORDER BY ... LIMIT 1
        Postgres parses ORDER BY as applying to the *whole* UNION unless each
        branch is wrapped in parentheses; when malformed, `EXPLAIN` raises a
        syntax error at or near "UNION".

    Strategy (fast, regex‑only — *not* a full SQL parser):
      1. Drop any trailing semicolon (gets in the way of concatenation).
      2. If the query contains UNION/UNION ALL *and* we detect an ORDER BY
         immediately before the UNION token, strip that ORDER BY…LIMIT fragment
         from the branch.  (Keeping it requires correct parenthesization; the
         model rarely emits it.)
      3. Return the cleaned SQL.  We do **not** attempt to re‑parenthesize,
         because naive wrapping can break CTEs; dropping the clause yields a
         valid, if slightly less selective, query that will at least parse so
         downstream scoring / regeneration can proceed.

    NOTE: This is intentionally conservative.  If you *must* preserve the
    per‑branch ORDER BY…LIMIT semantics, replace this function with a proper
    parser that wraps each branch: `(SELECT ... ORDER BY ... LIMIT n)`.

    Parameters
    ----------
    sql : str
        Raw SQL candidate from the LLM.

    Returns
    -------
    str
        Sanitized SQL safe(r) for EXPLAIN / execution.
    """
    if not isinstance(sql, str):
        return sql

    cleaned = sql.strip().rstrip(";")

    if "UNION" not in cleaned.upper():
        return cleaned

    # Strip ORDER BY...LIMIT fragments that immediately precede UNION tokens.
    cleaned = _ORDER_BEFORE_UNION_RE.sub("", cleaned)

    return cleaned


# Centralized SQL preparation pipeline
def prepare_sql_for_execution(
    sql: str, *, preview: bool = False, lint: bool = True
) -> str:
    """
    Prepare and normalize SQL string for execution.
    Applies all cleaning, normalization, and rewriting steps.
    """
    logger.info("prepare_sql_for_execution: initial input")
    logger.info(sql)

    # If sql is a list, take first element
    if isinstance(sql, list):
        sql = sql[0]

    # Remove full-line SQL comments
    sql = re.sub(r"(?m)^\s*--.*\n?", "", sql)

    # Strip whitespace and trailing semicolon
    sql = sql.strip().rstrip(";")

    # Preflight fix for LLM-generated UNION/UNION ALL
    sql = _fix_unions_for_preview(sql)

    # Enforce process_text usage on both sides of textual filters
    sql = enforce_process_text_filters(sql)

    # Apply country ISO fixes for specific cases
    for ctry in ["dom", "pan", "pry", "hnd"]:
        sql = sql.replace(f"'{ctry} '", f"'{ctry}'").replace(f" '{ctry}'", f"'{ctry}'")

    # Regex transforms:
    # ilike process_text('X') → ilike '%' || process_text('X') || '%'
    sql = _expand_process_text_matches(sql)

    sql = _replace_process_text_wildcards(sql)
    sql = _normalize_year_comparisons(sql)
    sql = _guard_division_by_zero(sql)

    logger.info("prepare_sql_for_execution: after regex transforms")
    logger.info(sql)

    # Apply normalization pipeline
    sql = normalize_territorial_expressions(sql)
    sql = remove_process_text_from_numeric_fields(sql)
    sql = replace_in_clause_ilike(sql)
    sql = expand_ilike_keywords(sql)
    sql = _ensure_order_before_limit(sql)
    sql = _maybe_apply_distinct_for_one_to_many(sql)
    sql = _fix_distinct_order_by_mismatch(sql)

    if lint:
        logger.info("prepare_sql_for_execution: lint_and_fix_sql (pre)")
        sql = lint_and_fix_sql(sql)
        logger.info("prepare_sql_for_execution: lint_and_fix_sql (post)")
        # Reaplica refuerzo de filtros textuales por si el linter alteró la sintaxis
        sql = enforce_process_text_filters(sql)

    sql = unwrap_process_text_with_alias(sql)

    # Fix malformed NULLIF rewritten as "NULLIF(SUM(...) AS NUMERIC)" by safety casts
    sql = _fix_malformed_nullif(sql)

    _warn_possible_missing_group_by(sql)

    logger.info("prepare_sql_for_execution: final output")
    logger.info(sql)

    return sql.strip().rstrip(";")


_YEAR_COMPARISON_RE = re.compile(
    r"(?P<col>\b[a-zA-Z_][\w\.]*?(?:fecha|fechafin|fechainicio|fecha_inicio|fecha_fin)[\w\.]*?)\s*"
    r"(?P<op>>=|<=|=|>|<)\s*(?P<year>\d{4})\b",
    flags=re.IGNORECASE,
)

_DIVISION_DEN_RE = re.compile(
    r"(?P<num>[^/]+?)\s*/\s*(?P<den>(?:COUNT|SUM)\s*\([^)]*\))",
    flags=re.IGNORECASE,
)

_PROCESS_TEXT_WILDCARD_RE = re.compile(
    r"process_text\((?P<expr>[^)]+)\)\s+ILIKE\s+'%\s*\|\|\s*process_text\('(?P<value>[^']+)'\)\s*\|\|\s*%'",
    flags=re.IGNORECASE,
)

_MALFORMED_NULLIF_RE = re.compile(
    r"NULLIF\(\s*(?P<agg>(?:SUM|COUNT)\s*\([^)]*\))\s+AS\s+NUMERIC\s*\)",
    flags=re.IGNORECASE | re.DOTALL,
)


def _fix_malformed_nullif(sql: str) -> str:
    """
    Repara casos donde una transformación previa dejó 'NULLIF(SUM(...) AS NUMERIC)'
    (perdiendo el segundo argumento). Lo vuelve a 'NULLIF(SUM(...), 0)'.
    """
    changed = False

    def _repl(match: re.Match) -> str:
        agg = match.group("agg").strip()
        nonlocal changed
        changed = True
        return f"NULLIF({agg}, 0)"

    fixed = _MALFORMED_NULLIF_RE.sub(_repl, sql)
    if changed:
        logger.info("SQL ▸ FIX malformed NULLIF applied")
    return fixed


def _normalize_year_comparisons(sql: str) -> str:
    """
    Transforma comparaciones como `col_fecha >= 2020` en `EXTRACT(YEAR FROM col_fecha) >= 2020`
    para evitar errores `date >= integer`.
    """

    def _replacer(match: re.Match) -> str:
        column = match.group("col")
        operator = match.group("op")
        year = match.group("year")

        if "EXTRACT" in column.upper():
            return match.group(0)

        col_lower = column.lower()
        if "anio" in col_lower or "year" in col_lower:
            logger.info(
                "\n\n ---> SQL ▸ HEURISTIC normalize_year integer column=%s operator=%s year=%s",
                column,
                operator,
                year,
            )
            return f"{column} {operator} {year}"

        logger.info(
            "\n\n ---> SQL ▸ HEURISTIC normalize_year column=%s operator=%s year=%s",
            column,
            operator,
            year,
        )

        return f"EXTRACT(YEAR FROM {column}) {operator} {year}"

    return _YEAR_COMPARISON_RE.sub(_replacer, sql)


def _expand_process_text_matches(sql: str) -> str:
    """
    Convierte comparaciones entre process_text(...) y valores literales
    a coincidencias parciales sólo cuando no se trata de tokens ya
    expandibles (por ejemplo, w.token en el fallback).
    """

    def _should_skip(expr: str) -> bool:
        expr_lower = expr.lower()
        return "w.token" in expr_lower or "regexp_split_to_table" in expr_lower

    def _repl_ilike_direct(match: re.Match) -> str:
        value = match.group(1)
        return f"ilike '%' || process_text('{value}') || '%'"

    def _repl_eq(match: re.Match) -> str:
        expr = match.group("expr").strip()
        value = match.group("value")
        if _should_skip(expr):
            return match.group(0)
        return f"process_text({expr}) ILIKE '%' || process_text('{value}') || '%'"

    def _repl_ilike(match: re.Match) -> str:
        expr = match.group("expr").strip()
        value = match.group("value")
        if _should_skip(expr):
            return match.group(0)
        return f"process_text({expr}) ILIKE '%' || process_text('{value}') || '%'"

    sql = re.sub(
        r"ilike process_text\('([^']+)'\)",
        _repl_ilike_direct,
        sql,
        flags=re.IGNORECASE,
    )

    sql = re.sub(
        r"process_text\((?P<expr>[^)]+)\)\s*=\s*process_text\('(?P<value>[^']+)'\)",
        _repl_eq,
        sql,
        flags=re.IGNORECASE,
    )

    sql = re.sub(
        r"process_text\((?P<expr>[^)]+)\)\s+ILIKE\s+process_text\('(?P<value>[^']+)'\)",
        _repl_ilike,
        sql,
        flags=re.IGNORECASE,
    )

    return sql


def _guard_division_by_zero(sql: str) -> str:
    """Wrap COUNT/SUM denominators in NULLIF to avoid division by zero."""

    def _replacer(match: re.Match) -> str:
        numerator = match.group("num").rstrip()
        denominator = match.group("den")

        # Si la expresión ya contiene NULLIF/CAST, no tocarla
        if "NULLIF" in match.group(0).upper():
            return match.group(0)
        if "NULLIF" in denominator.upper():
            return match.group(0)

        logger.info(
            "\n\n ---> SQL ▸ HEURISTIC division_guard numerator=%s denominator=%s",
            numerator.strip(),
            denominator.strip(),
        )
        return f"{numerator} / NULLIF({denominator}, 0)"

    return _DIVISION_DEN_RE.sub(_replacer, sql)


def _warn_possible_missing_group_by(sql: str) -> None:
    upper_sql = sql.upper()
    if (
        "SUM(" in upper_sql or "COUNT(" in upper_sql or "AVG(" in upper_sql
    ) and "GROUP BY" not in upper_sql:
        logger.warning("\n\n ---> SQL ▸ HEURISTIC potential_missing_group_by detected")


def _normalize_table_name(table_name: str) -> str:
    if not table_name:
        return ""
    return table_name.strip().strip('"').lower()


def _table_name_variants(table_name: str) -> Set[str]:
    norm = _normalize_table_name(table_name)
    if not norm:
        return set()
    parts = norm.split(".")
    variants = {norm}
    if parts:
        variants.add(parts[-1])
    return variants


_RELATIONS_CACHE: Optional[
    Tuple[Dict[str, Set[str]], Dict[Tuple[str, str], Dict[str, str]]]
] = None

_LIMIT_OFFSET_RE = re.compile(r"(?is)\s+limit\s+\d+(?:\s+offset\s+\d+)?\s*$")
_FETCH_FIRST_RE = re.compile(r"(?is)\s+fetch\s+first\s+\d+\s+rows?\s+only\s*$")


def _strip_trailing_limit_offset_fetch(sql: str) -> str:
    """
    Removes trailing LIMIT/OFFSET or FETCH FIRST clauses from the statement.
    """
    trimmed = sql.rstrip().rstrip(";")
    previous = None
    while trimmed and trimmed != previous:
        previous = trimmed
        trimmed = _LIMIT_OFFSET_RE.sub("", trimmed).rstrip()
        trimmed = _FETCH_FIRST_RE.sub("", trimmed).rstrip()
    return trimmed


def _find_last_top_level_order_by(sql: str) -> int:
    """
    Returns the start index of the last top-level ORDER BY (depth 0), or -1 if none.
    """
    lower = sql.lower()
    depth = 0
    idx = -1
    i = 0
    length = len(sql)
    while i < length:
        ch = lower[i]
        if ch == "(":
            depth += 1
        elif ch == ")":
            depth = max(0, depth - 1)
        elif depth == 0 and lower.startswith("order by", i):
            # ensure word boundary at left
            if i == 0 or not lower[i - 1].isalnum():
                idx = i
        i += 1
    return idx


def _strip_trailing_order_by(sql: str) -> str:
    trimmed = sql.rstrip()
    idx = _find_last_top_level_order_by(trimmed)
    if idx == -1:
        return trimmed
    return trimmed[:idx].rstrip()


def _prepare_count_subquery(sql: str) -> Optional[str]:
    """
    Generates the body suitable for wrapping in SELECT COUNT(*) FROM (...).
    Returns None if unable to prepare safely.
    """
    if not sql:
        return None
    cleaned = _strip_trailing_limit_offset_fetch(sql)
    cleaned = _strip_trailing_order_by(cleaned)
    cleaned = cleaned.strip().rstrip(";")
    if not cleaned:
        return None
    return cleaned


def load_relations_map(
    refresh: bool = False,
) -> Tuple[Dict[str, Set[str]], Dict[Tuple[str, str], Dict[str, str]]]:
    """
    Load 1:M relations from sch_relations and cache them. Returns:
      - relations: {one_table_variant -> {many_table_variant}}
      - columns: {(one_variant, many_variant) -> {"one_column": ..., "many_column": ...}}
    """
    global _RELATIONS_CACHE
    if not refresh and _RELATIONS_CACHE is not None:
        return _RELATIONS_CACHE

    relations: Dict[str, Set[str]] = {}
    columns: Dict[Tuple[str, str], Dict[str, str]] = {}
    query = text(
        "SELECT source_table, source_column, target_table, target_column, relationship_type "
        "FROM sch_relations WHERE is_valid = TRUE"
    )
    try:
        with engine.connect() as conn:
            rows = conn.execute(query).fetchall()
    except Exception as exc:
        logger.warning(
            "\n\n ---> SQL ▸ RELATIONS unable to load sch_relations: %s", exc
        )
        return _RELATIONS_CACHE or (relations, columns)

    for row in rows:
        try:
            rel_type = (row["relationship_type"] or "").strip().lower()
            if not rel_type.startswith("1:m"):
                continue
            source_table = row["source_table"]
            target_table = row["target_table"]
            source_column = (row["source_column"] or "").strip()
            target_column = (row["target_column"] or "").strip()
        except Exception:
            continue

        one_variants = _table_name_variants(target_table)
        many_variants = _table_name_variants(source_table)
        if not one_variants or not many_variants:
            continue

        for one in one_variants:
            rel_set = relations.setdefault(one, set())
            for many in many_variants:
                rel_set.add(many)
                if target_column or source_column:
                    columns[(one, many)] = {
                        "one_column": target_column,
                        "many_column": source_column,
                    }
    _RELATIONS_CACHE = (relations, columns)
    return _RELATIONS_CACHE


_FROM_JOIN_PATTERN = re.compile(
    r"\b(from|join)\s+([A-Za-z_][\w\.]*)(?:\s+(?:as\s+)?([A-Za-z_][\w]*))?",
    flags=re.IGNORECASE,
)


def _extract_alias_mapping(
    sql: str,
) -> Tuple[Optional[str], Optional[str], Dict[str, str]]:
    """
    Identify main table aliasing in the outermost SELECT.
    Returns (base_table, base_alias, {alias: table_name})
    """
    main_idx = _find_main_select_index(sql)
    if main_idx is None:
        return None, None, {}

    snippet = sql[main_idx:]
    alias_map: Dict[str, str] = {}
    base_alias = None
    base_table = None

    for match in _FROM_JOIN_PATTERN.finditer(snippet):
        keyword = match.group(1).lower()
        table = match.group(2)
        alias = match.group(3)
        normalized_table = _normalize_table_name(table)
        alias_clean = alias or table.split(".")[-1]
        alias_map[alias_clean.lower()] = normalized_table
        if keyword == "from" and base_alias is None:
            base_alias = alias_clean.lower()
            base_table = normalized_table

    return base_table, base_alias, alias_map


def _select_references_only_alias(sql: str, allowed_aliases: Set[str]) -> bool:
    match = _SELECT_CLAUSE.search(sql)
    if not match:
        return True
    body = match.group("body")
    for alias in re.findall(r"\b([A-Za-z_][\w]*)\s*\.", body):
        if alias.lower() not in allowed_aliases:
            return False
    return True


def _maybe_apply_distinct_for_one_to_many(sql: str) -> str:
    """
    Heurística defensiva: cuando detectamos joins donde la tabla principal está en
    el lado 1 y se proyectan sólo columnas del maestro, aplicamos DISTINCT.
    """
    lowered = sql.lower()
    if " join " not in lowered:
        return sql
    if re.search(r"\bselect\s+distinct\b", lowered):
        return sql
    if " group by " in lowered:
        return sql
    if re.search(r"\bdistinct\s+on\b", lowered):
        return sql
    if re.search(r"\bunion\b|\bintersect\b|\bexcept\b", lowered):
        return sql
    if re.search(r"\b(sum|avg|count|min|max|array_agg|string_agg)\s*\(", lowered):
        return sql

    base_table, base_alias, alias_map = _extract_alias_mapping(sql)
    if not base_table or not alias_map:
        return sql

    relations_map, _ = load_relations_map()
    base_variants = _table_name_variants(base_table)
    if not base_variants:
        return sql

    # Determine if any joined table is registered as many-side relative to base.
    candidate_aliases: List[str] = []
    for alias, table in alias_map.items():
        if base_alias and alias == base_alias:
            continue
        table_variants = _table_name_variants(table)
        found_relation = any(
            many in relations_map.get(one, set())
            for one in base_variants
            for many in table_variants
        )
        if found_relation:
            candidate_aliases.append(alias)

    if not candidate_aliases:
        return sql

    allowed_aliases = set(base_variants)
    if base_alias:
        allowed_aliases.add(base_alias)

    if not _select_references_only_alias(sql, allowed_aliases):
        logger.info(
            "\n\n ---> SQL ▸ HEURISTIC skip DISTINCT because SELECT references joined-table columns"
        )
        return sql

    main_select_idx = _find_main_select_index(sql)
    if main_select_idx is None:
        return sql

    snippet = sql[main_select_idx : main_select_idx + 16]
    if re.search(r"select\s+distinct", snippet, re.IGNORECASE):
        return sql

    logger.info("\n\n ---> SQL ▸ HEURISTIC add DISTINCT on detected 1:M join")
    return f"{sql[:main_select_idx + len('select')]} DISTINCT{sql[main_select_idx + len('select'):]}"


def _find_main_select_index(sql: str) -> Optional[int]:
    """
    Localiza el SELECT principal (fuera de CTEs) para insertar DISTINCT.
    Devuelve None si no se encuentra una coincidencia confiable.
    """
    stripped = sql.lstrip()
    offset = len(sql) - len(stripped)
    text = stripped

    if not text.lower().startswith("with"):
        match = re.search(r"\bselect\b", text, re.IGNORECASE)
        return offset + match.start() if match else None

    depth = 0
    i = 0
    length = len(text)
    while i < length:
        ch = text[i]
        if ch == "(":
            depth += 1
        elif ch == ")":
            depth = max(depth - 1, 0)
        elif depth == 0 and text[i : i + 6].lower() == "select":
            return offset + i
        i += 1
    return None


def _fix_distinct_order_by_mismatch(sql: str) -> str:
    """
    PostgreSQL requiere que todas las columnas en ORDER BY estén en SELECT
    cuando se usa SELECT DISTINCT. Esta función detecta y corrige el problema
    eliminando el ORDER BY si contiene columnas no seleccionadas.

    Ejemplo problemático:
        SELECT DISTINCT p.id_proyecto, p.nombre_proyecto
        FROM proyectos p
        ORDER BY p.valor_proyecto DESC  -- ERROR: valor_proyecto no está en SELECT
        LIMIT 11

    Se corrige a:
        SELECT DISTINCT p.id_proyecto, p.nombre_proyecto
        FROM proyectos p
        LIMIT 11
    """
    lowered = sql.lower()

    # Solo actúa si hay SELECT DISTINCT
    if not re.search(r"\bselect\s+distinct\b", lowered):
        return sql

    # No procesar si hay subqueries complejas, UNION, etc.
    if re.search(r"\bunion\b|\bintersect\b|\bexcept\b", lowered):
        return sql

    # Verificar si hay ORDER BY
    order_match = re.search(r"\border\s+by\s+", lowered)
    if not order_match:
        return sql

    try:
        # Extraer columnas del SELECT (entre SELECT DISTINCT y FROM)
        select_match = re.search(
            r"\bselect\s+distinct\s+(.+?)\s+from\s+", sql, re.IGNORECASE | re.DOTALL
        )
        if not select_match:
            return sql

        select_cols_raw = select_match.group(1)

        # Extraer el ORDER BY clause (hasta LIMIT, OFFSET, fin, o próximo clause)
        order_match_full = re.search(
            r"\border\s+by\s+(.+?)(?:\s+limit\s+|\s+offset\s+|\s+fetch\s+|$)",
            sql,
            re.IGNORECASE | re.DOTALL,
        )
        if not order_match_full:
            return sql

        order_cols_raw = order_match_full.group(1)

        # Parsear columnas del SELECT (manejar aliases y funciones)
        # Extraemos los identificadores finales (alias o columna sin alias)
        def extract_select_identifiers(cols_str: str) -> set:
            """Extrae los identificadores disponibles del SELECT."""
            identifiers = set()
            # Split por comas pero respetando paréntesis
            depth = 0
            current = []
            for char in cols_str:
                if char == "(":
                    depth += 1
                    current.append(char)
                elif char == ")":
                    depth -= 1
                    current.append(char)
                elif char == "," and depth == 0:
                    col = "".join(current).strip()
                    if col:
                        # Extraer alias (AS name) o la columna misma
                        alias_match = re.search(
                            r"\s+as\s+(\w+)\s*$", col, re.IGNORECASE
                        )
                        if alias_match:
                            identifiers.add(alias_match.group(1).lower())
                        else:
                            # Usar la columna completa (ej: p.valor_proyecto)
                            # Y también el nombre sin tabla (valor_proyecto)
                            col_clean = col.strip().rstrip(",")
                            identifiers.add(col_clean.lower())
                            if "." in col_clean:
                                identifiers.add(col_clean.split(".")[-1].lower())
                    current = []
                else:
                    current.append(char)
            # Última columna
            col = "".join(current).strip()
            if col:
                alias_match = re.search(r"\s+as\s+(\w+)\s*$", col, re.IGNORECASE)
                if alias_match:
                    identifiers.add(alias_match.group(1).lower())
                else:
                    col_clean = col.strip()
                    identifiers.add(col_clean.lower())
                    if "." in col_clean:
                        identifiers.add(col_clean.split(".")[-1].lower())
            return identifiers

        def extract_order_columns(order_str: str) -> list:
            """Extrae las columnas del ORDER BY."""
            cols = []
            depth = 0
            current = []
            for char in order_str:
                if char == "(":
                    depth += 1
                    current.append(char)
                elif char == ")":
                    depth -= 1
                    current.append(char)
                elif char == "," and depth == 0:
                    col = "".join(current).strip()
                    if col:
                        # Remover ASC/DESC
                        col_clean = re.sub(
                            r"\s+(asc|desc)\s*$", "", col, flags=re.IGNORECASE
                        ).strip()
                        cols.append(col_clean)
                    current = []
                else:
                    current.append(char)
            # Última columna
            col = "".join(current).strip()
            if col:
                col_clean = re.sub(
                    r"\s+(asc|desc)\s*$", "", col, flags=re.IGNORECASE
                ).strip()
                cols.append(col_clean)
            return cols

        select_ids = extract_select_identifiers(select_cols_raw)
        order_cols = extract_order_columns(order_cols_raw)

        # Verificar si todas las columnas del ORDER BY están en SELECT
        missing_cols = []
        for ocol in order_cols:
            ocol_lower = ocol.lower()
            ocol_base = ocol.split(".")[-1].lower() if "." in ocol else ocol_lower
            # Verificar si la columna o su base está en el SELECT
            if not (ocol_lower in select_ids or ocol_base in select_ids):
                missing_cols.append(ocol)

        if missing_cols:
            logger.warning(
                f"_fix_distinct_order_by_mismatch: ORDER BY columns {missing_cols} "
                f"not in SELECT DISTINCT. Removing ORDER BY clause."
            )
            # Remover el ORDER BY clause completo
            # Patrón: ORDER BY ... (hasta LIMIT, OFFSET, o fin)
            sql = re.sub(
                r"\s+order\s+by\s+.+?(?=\s+limit\s+|\s+offset\s+|\s+fetch\s+|$)",
                "",
                sql,
                flags=re.IGNORECASE | re.DOTALL,
            )
            logger.info(f"_fix_distinct_order_by_mismatch: Fixed SQL:\n{sql}")

        return sql

    except Exception as e:
        logger.warning(f"_fix_distinct_order_by_mismatch: Error processing SQL: {e}")
        return sql


def _replace_process_text_wildcards(sql: str) -> str:
    """
    Legacy function - now returns SQL unchanged.
    The ILIKE pattern with process_text is the correct approach.
    """
    # Ya no convertimos a regexp_split_to_table - ILIKE con process_text es el patrón correcto
    return sql


def execute_sql_query(
    db: Session,
    sql_query,
    rows_limit_default=settings.sql_rows_limit,
    rows_limit_max=settings.sql_rows_limit_max,
):
    """
    Ejecuta una consulta SQL en PostgreSQL y maneja SELECT, INSERT y UPDATE.
    También transforma expresiones exactas con `process_text` a búsquedas parciales con ILIKE.
    """

    # ensure limits are ints (or None)
    try:
        rows_limit_default = (
            int(rows_limit_default) if rows_limit_default is not None else None
        )
    except (TypeError, ValueError):
        rows_limit_default = settings.sql_rows_limit
    if rows_limit_default is not None and rows_limit_default <= 0:
        rows_limit_default = None

    try:
        rows_limit_max = int(rows_limit_max) if rows_limit_max is not None else None
    except (TypeError, ValueError):
        rows_limit_max = settings.sql_rows_limit_max
    if rows_limit_max is not None and rows_limit_max <= 0:
        rows_limit_max = None
    if (
        rows_limit_default is not None
        and rows_limit_max is not None
        and rows_limit_default > rows_limit_max
    ):
        rows_limit_default = rows_limit_max

    try:
        # Limpieza defensiva: si la conexión viene de un error previo, revertirla
        _reset_failed_transaction(db)

        if isinstance(sql_query, list):
            if not sql_query:
                logger.error(
                    f"Error en `execute_sql_query()`: sql_query es lista vacía"
                )
                raise ValueError("sql_query es una lista vacía - no se puede ejecutar")
            sql_query = sql_query[0]

        if not sql_query or not isinstance(sql_query, str):
            logger.error(
                f"Error en `execute_sql_query()`: sql_query inválido type={type(sql_query)} value={sql_query}"
            )
            raise ValueError(
                f"sql_query debe ser string no vacío, recibido: {type(sql_query)}"
            )

        stripped_for_type = sql_query.strip()
        if not stripped_for_type:
            raise ValueError("sql_query no puede estar vacío.")

        query_type = stripped_for_type.split()[0].upper()
        _enforce_sql_guardrails(sql_query, query_type)

        # Remove full-line SQL comments
        sql_query = re.sub(r"(?m)^\s*--.*\n?", "", sql_query).strip()

        if query_type == "INSERT":
            logger.info(f"Ejecutando INSERT: {sql_query}")
            result = db.execute(text(sql_query))
            db.commit()

            inserted_id_row = result.fetchone()
            answer_id = inserted_id_row[0] if inserted_id_row else None
            return {"answer_id": answer_id}, False, 1 if answer_id else 0, 0

        elif query_type == "UPDATE":
            logger.info(f"Ejecutando UPDATE: {sql_query}")
            result = db.execute(text(sql_query))
            db.commit()
            return {"rows_affected": result.rowcount}, False, result.rowcount, 0

        elif query_type in _READ_QUERY_TYPES:
            # Centralized SQL preparation pipeline
            logger.info("--------PRE SQL (raw) --------")
            logger.info(sql_query)
            sql_query = prepare_sql_for_execution(sql_query, preview=False, lint=True)
            logger.info("--------POST SQL (clean)------")
            logger.info(sql_query)
            limit_effective: Optional[int] = None
            original_limit: Optional[int] = None
            offset_value: Optional[int] = None

            limit_match = re.search(
                r"(?is)\blimit\s+(\d+)(?:\s+offset\s+(\d+))?\s*$", sql_query
            )
            executed_query = sql_query
            limit_effective = None
            if limit_match:
                try:
                    original_limit = int(limit_match.group(1))
                except (TypeError, ValueError):
                    original_limit = None
                try:
                    offset_value = (
                        int(limit_match.group(2)) if limit_match.group(2) else None
                    )
                except (TypeError, ValueError):
                    offset_value = None

                # Límites genéricos que el LLM pone por defecto (no solicitados por usuario)
                LLM_GENERIC_LIMITS = {50, 100, 200, 500, 1000}

                if original_limit is not None:
                    limit_effective = original_limit
                    # Si el LLM puso un límite genérico, aplicar el default configurado
                    if (
                        original_limit in LLM_GENERIC_LIMITS
                        and rows_limit_default is not None
                    ):
                        logger.info(
                            "SQL ▸ EXEC llm_generic_limit detected=%s using_default=%s",
                            original_limit,
                            rows_limit_default,
                        )
                        limit_effective = rows_limit_default
                    # Siempre respetar el tope máximo
                    if rows_limit_max is not None:
                        limit_effective = min(limit_effective, rows_limit_max)
                    if limit_effective is not None:
                        fetch_limit = limit_effective + 1
                        replacement = f"LIMIT {fetch_limit}"
                        if offset_value is not None:
                            replacement += f" OFFSET {offset_value}"
                        executed_query = (
                            sql_query[: limit_match.start()]
                            + replacement
                            + sql_query[limit_match.end() :]
                        )
                else:
                    if rows_limit_default is not None:
                        limit_effective = rows_limit_default
                        fetch_limit = limit_effective + 1
                        replacement = f"LIMIT {fetch_limit}"
                        if offset_value is not None:
                            replacement += f" OFFSET {offset_value}"
                        executed_query = (
                            sql_query[: limit_match.start()]
                            + replacement
                            + sql_query[limit_match.end() :]
                        )
            else:
                if rows_limit_default is not None:
                    limit_effective = rows_limit_default
                    fetch_limit = limit_effective + 1
                    executed_query = f"{sql_query.rstrip(';')} LIMIT {fetch_limit}"

            logger.info(f"Ejecutando SELECT: {executed_query}")
            # Use AUTOCOMMIT engine to prevent IdleInTransactionSessionTimeout
            # when LLM calls block between DB operations
            with sync_db_acquire():
                with chatbot_engine.connect() as conn:
                    result = conn.execute(text(executed_query))
                    columns = result.keys()
                    rows = result.fetchall()
            total_rows_raw = len(rows)

            applied_limit = None
            if limit_effective is not None:
                applied_limit = limit_effective
                if total_rows_raw > applied_limit:
                    rows = rows[:applied_limit]

            if total_rows_raw == 0:
                return [], False, 0, 0, 0

            actual_total: Optional[int] = None
            more_than_n_rows = False

            # Si hay límite (ya sea impuesto por el LLM o por el default), calcular el total real
            if limit_effective is not None:
                try:
                    count_sql_body = _prepare_count_subquery(sql_query)
                    if count_sql_body:
                        count_query = (
                            f"SELECT COUNT(*) FROM ({count_sql_body}) AS total_subquery"
                        )
                        # Use AUTOCOMMIT engine for count query too
                        with chatbot_engine.connect() as conn:
                            count_result = conn.execute(text(count_query)).scalar()
                        if count_result is not None:
                            actual_total = int(count_result)
                            more_than_n_rows = actual_total > len(rows)
                except Exception as exc:
                    logger.warning("\n\n ---> SQL ▸ COUNT fallback failed {}", exc)
                    # No need to rollback - using AUTOCOMMIT engine

            if actual_total is None and applied_limit:
                more_than_n_rows = total_rows_raw > applied_limit

            total_rows_displayed = len(rows)
            results_list = []
            for row in rows:
                mapped = {}
                for column in columns:
                    value = row._mapping[column]
                    if value is None:
                        mapped[column] = ""
                    else:
                        mapped[column] = str(value)
                results_list.append(mapped)

            if total_rows_displayed <= 1 and not any(results_list[0].values()):
                no_data = 1
            else:
                no_data = 0

            # Enriquecer con HTML renderizado cuando no exista traza guardada
            for item in results_list:
                if item.get("Trace"):
                    continue
                answer_text = item.get("Answer") or ""
                if answer_text.strip():
                    try:
                        item["Trace"] = markdown_to_html(answer_text)
                    except Exception as exc:  # pragma: no cover
                        logger.warning(
                            f"Fallo renderizando historial Markdown → HTML: {exc}"
                        )

            results_json = json.dumps(results_list, ensure_ascii=False, indent=4)
            results_json = (
                results_json.replace("\n", "")
                .replace("\t", "")
                .replace("\r", "")
                .replace("  ", " ")
            )

            total_rows_reported = (
                actual_total if actual_total is not None else total_rows_displayed
            )

            return (
                results_json,
                more_than_n_rows,
                total_rows_reported,
                no_data,
                total_rows_displayed,
            )

        else:
            raise ValueError(f"Tipo de consulta no soportado: {query_type}")

    except Exception as e:
        try:
            logger.error(
                "SQL ▸ EXEC failed sql={} error={}",
                locals().get("executed_query", sql_query),
                e,
                exc_info=True,
            )
        except Exception:
            logger.error("Error en `execute_sql_query()`: {}", e, exc_info=True)
        db.rollback()
        raise e


def _safe_row_limit(value: Optional[int], default: int = 15, maximum: int = 200) -> int:
    try:
        if value is None:
            return default
        val = int(value)
        return max(1, min(val, maximum))
    except (TypeError, ValueError):
        return default


def _normalize_session_id(session_id: str) -> str:
    if not session_id:
        return ""
    cleaned = re.sub(r"[^A-Za-z0-9_\-]", "", str(session_id))
    return cleaned[:128]


def get_question_history_by_sessionId(sessionId: str, row_limit=15):
    """
    Obtiene el historial de preguntas por `sessionId`.
    """
    normalized_session = _normalize_session_id(sessionId)
    if not normalized_session:
        logger.warning(
            "get_question_history_by_sessionId recibió sessionId vacío/invalid."
        )
        return None, False, 0, 0

    effective_limit = _safe_row_limit(row_limit)
    try:
        with Session() as db:
            sql_query = text(
                """
                SELECT 
                    Id,
                    Question,
                    Answer,
                    Trace,
                    Source,
                    QuestionSummary,
                    IsApproved,
                    CommentDisApproved,
                    Date,
                    UserSession,
                    pais
                FROM 
                    questions_mapainv_chat
                WHERE 
                    UserSession = :session_id
                ORDER BY 
                    Id DESC
                LIMIT :limit
                """
            )
            result = db.execute(
                sql_query,
                {"session_id": normalized_session, "limit": effective_limit},
            ).fetchall()

            if not result:
                logger.info(
                    f"No se encontraron datos para sessionId: {normalized_session}"
                )
                return [], False, 0, 0

            rows: List[Dict[str, Any]] = []
            for row in result:
                mapping = dict(row._mapping)
                for key, value in list(mapping.items()):
                    if isinstance(value, datetime):
                        mapping[key] = value.isoformat()
                rows.append(mapping)
            total_rows = len(rows)
            more_than_limit = total_rows >= effective_limit
            no_data = 0 if rows else 1
            return rows, more_than_limit, total_rows, no_data

    except Exception as err:
        logger.error(f"Error en `get_question_history_by_sessionId()`: {err}")
        return None, False, 0, 0


# ─────────────────────────────────────────────────────────────────────────────
# Analyzer-driven Semantic OR (from groups)
# Consolida múltiples bloques de keywords (p.nombre_proyecto/objetivo_proyecto)
# en AND (...) + OR internos, usando los literales detectados por el analyzer.
# Se activa SOLO si se proveen grupos desde el analyzer.
# ─────────────────────────────────────────────────────────────────────────────


def _build_sem_or_block_from_literals(literals: List[str]) -> str:
    parts = []
    for lit in literals:
        safe = (lit or "").replace("'", "''")
        parts.append(
            "("
            "process_text(p.nombre_proyecto) ILIKE '%' || process_text('"
            + safe
            + "') || '%' "
            "OR process_text(p.objetivo_proyecto) ILIKE '%' || process_text('"
            + safe
            + "') || '%'"
            ")"
        )
    return "(" + " OR ".join(parts) + ")"


def _find_kw_block_for_literal(sql: str, lit: str) -> Optional[Tuple[int, int]]:
    """
    Busca un bloque `AND ( ... )` que contenga process_text('lit') y haga referencia
    a p.nombre_proyecto / p.objetivo_proyecto dentro. Devuelve (start, end) del bloque completo.
    """
    if not lit:
        return None
    lit_esc = re.escape(lit)
    # AND ( ... process_text(p.nombre|objetivo) ... process_text('lit') ... )
    pat = re.compile(
        rf"\s+AND\s+\((?:(?:(?!\)).)*process_text\(\s*p\.(?:nombre_proyecto|objetivo_proyecto)\s*\)(?:(?!\)).)*process_text\('\s*{lit_esc}\s*'\)(?:(?!\)).)*|(?:(?:(?!\)).)*process_text\('\s*{lit_esc}\s*'\)(?:(?!\)).)*process_text\(\s*p\.(?:nombre_proyecto|objetivo_proyecto)\s*\)(?:(?!\)).)*)\)",
        re.IGNORECASE | re.DOTALL,
    )
    m = pat.search(sql)
    if not m:
        return None
    return m.start(), m.end()


def apply_semantic_or_from_groups(
    sql_query: str, semantic_groups: List[Dict[str, Any]]
) -> Tuple[str, bool]:
    """
    Usa los grupos sugeridos por el analyzer para colapsar varios bloques AND de keywords
    en un solo bloque OR interno. Devuelve (new_sql, changed).
    Solo actúa sobre p.nombre_proyecto / p.objetivo_proyecto.
    """
    try:
        groups = semantic_groups or []
        changed = False
        new_sql = sql_query

        for g in groups:
            literals = [
                str(x).strip() for x in (g.get("literals") or []) if str(x).strip()
            ]
            if len(literals) < 2:
                continue

            # Encontrar los bloques presentes para cada literal
            spans: List[Tuple[int, int]] = []
            for lit in literals:
                span = _find_kw_block_for_literal(new_sql, lit)
                if span:
                    spans.append(span)

            # Necesitamos al menos 2 bloques para colapsar
            if len(spans) < 2:
                continue

            spans = sorted(spans, key=lambda s: s[0])
            combined_block = " AND " + _build_sem_or_block_from_literals(literals)

            # Reescritura: reemplazar el primer bloque por el combinado y eliminar los demás
            pieces: List[str] = []
            cursor = 0
            first_start = spans[0][0]
            pieces.append(new_sql[:first_start])
            pieces.append(combined_block)
            cursor = spans[0][1]
            for st, en in spans[1:]:
                pieces.append(new_sql[cursor:st])
                cursor = en
            pieces.append(new_sql[cursor:])
            new_sql = "".join(pieces)
            changed = True

        return new_sql, changed
    except Exception as e:
        try:
            logger.warning(
                f"\n\n ---> SQL ▸ SEMANTIC_OR(from analyzer) skipped due to error: {e}"
            )
        except Exception:
            pass
        return sql_query, False


# Exported symbols
__all__ = [
    "execute_sql_query",
    "prepare_sql_for_execution",
    "lint_and_fix_sql",
    "normalize_territorial_expressions",
    "remove_process_text_from_numeric_fields",
    "replace_in_clause_ilike",
    "expand_ilike_keywords",
    "unwrap_process_text_with_alias",
    "strip_process_text_in_select",
    "enforce_process_text_filters",
    "load_relations_map",
    "get_table_name",
    "get_question_history_by_sessionId",
    "add_question",
    "apply_semantic_or_from_groups",
]
